{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b884ef-5c4f-46fb-b9cb-022019c28179",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-storage google-cloud-bigquery\n",
    "!pip install pandas\n",
    "!pip install pandas-gbq\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c488a8-0aee-4ec4-9a8f-0c47116131dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas_gbq import to_gbq\n",
    "import numpy as np\n",
    "import json\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from google.cloud import bigquery\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea24bb-deff-474d-a654-79705045a3d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id=\"your_project_id\"  # Replace with your project ID\n",
    "dataset_id=\"rag_dataset\"\n",
    "table_id=\"disease_embeddings\"\n",
    "\n",
    "dataset_source=\"https://huggingface.co/datasets/noobmaster1246/disease_prediction/resolve/main/embeddings.tar.xz\"\n",
    "embeddings_dataset_compress=\"embeddings.tar.xz\"\n",
    "embeddings_dataset=\"embeddings.bin\"\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-base\")\n",
    "gemini_model = GenerativeModel(\"gemini-2.0-flash\")\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563a926-9f52-4c34-a10a-4632ac533ba8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if embeddings_dataset in os.listdir():\n",
    "    print(f\"{embeddings_dataset} found\")\n",
    "else:\n",
    "    if embeddings_dataset_compress in os.listdir():\n",
    "        print(f\"{embeddings_dataset_compress} is in the folder.\")\n",
    "        print(\"decompressing to\", end=\" \")\n",
    "        subprocess.run([\"tar\", \"-xvJf\", embeddings_dataset_compress], check=True)\n",
    "    else:\n",
    "        print(f\"{embeddings_dataset_compress} is not in the folder.\")\n",
    "        print(\"downloading...\")\n",
    "        subprocess.run([\"wget\", dataset_source], check=True)\n",
    "        print(\"decompressing to\", end=\" \")\n",
    "        subprocess.run([\"tar\", \"-xvJf\", embeddings_dataset_compress], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2c5ee-e870-419d-a346-d58e211c8ab7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run only once to load the embeddings from the file to big query\n",
    "\n",
    "df = pd.DataFrame(pd.read_pickle(embeddings_dataset))\n",
    "df['embedding'] = df['embedding'].apply(list)\n",
    "client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"disease\", \"STRING\", mode=\"REQUIRED\"),    \n",
    "    bigquery.SchemaField(\"text\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"embedding\", \"FLOAT\", mode=\"REPEATED\")\n",
    "]\n",
    "\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "try:\n",
    "    client.get_table(table_ref)\n",
    "    print(\"Table already exists. Skipping creation.\")\n",
    "except:\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    client.create_table(table)\n",
    "    print(f\"Table {table_id} created in dataset {dataset_id}.\")\n",
    "    \n",
    "chunk_size = 10000 #change based on api limit\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df[i:i + chunk_size]\n",
    "    chunk.to_gbq(\n",
    "        destination_table=f\"{dataset_id}.{table_id}\",\n",
    "        project_id=project_id,\n",
    "        if_exists=\"append\" if i > 0 else \"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901022e-c37c-44ba-9a4d-29ad44a73f78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_similar_cases(query_embedding, top_k=1000):\n",
    "    embedding_sql = \"[\" + \", \".join(f\"{x:.6f}\" for x in query_embedding) + \"]\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT disease, text \n",
    "        FROM `{project_id}.{dataset_id}.disease_embeddings`\n",
    "        ORDER BY ML.DISTANCE(embedding, {embedding_sql}, 'COSINE')\n",
    "        LIMIT {top_k}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = client.query(query).result()\n",
    "    return [dict(row) for row in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b8484-d153-46f7-b97a-dffbf6a499fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_query_embedding(user_input):\n",
    "    inputs = f\"[CLS] {user_input}\"\n",
    "    \n",
    "    embedding = model.encode(inputs)\n",
    "    \n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d3ad3-9689-47d4-a088-5d30f8e6ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disease(user_input):\n",
    "    query_embedding = generate_query_embedding(user_input)\n",
    "\n",
    "    cases = retrieve_similar_cases(query_embedding)\n",
    "\n",
    "    context = \"\\n\".join([f\"- Disease: {case['disease']}, Description: {case['text']}\" for case in cases])\n",
    "\n",
    "    prompt = f\"\"\"**Your Role:** You are 'QuickAID', an AI health assistant. Your purpose is to understand user-reported symptoms and, using the provided context cases, suggest *potential* related conditions. You must be empathetic and helpful. You will respond with *only* a valid JSON object containing 'response', 'thinking', and 'reasoning' fields.\n",
    "\n",
    "**Critical Limitations:**\n",
    "* You are NOT a substitute for a professional medical diagnosis or advice.\n",
    "* You cannot provide medical treatment recommendations.\n",
    "* Your knowledge is based on the provided context and general patterns.\n",
    "\n",
    "**Context Cases:** Below are descriptions/symptoms associated with certain diseases. These *might* be relevant to the user's situation. Use them cautiously as reference points.\n",
    "--- START CONTEXT ---\n",
    "{context}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "**User's Input:**\n",
    "\"{user_input}\"\n",
    "\n",
    "**Your Task and Response Logic:**\n",
    "\n",
    "1. **Analyze Input:** First, determine the nature of the user's input.\n",
    "    * Does it clearly describe specific physical or mental health symptoms?\n",
    "    * Is it vague or general?\n",
    "    * Is it conversational or unrelated to health?\n",
    "    * Do use your own knowledge besides the context.\n",
    "    * Ask as many questions you need to ask to the user.\n",
    "    * Identify yourself as QuickAID.\n",
    "    * Do not hallucinate.\n",
    "    * Do not produce bad or unrelated output.\n",
    "    * Try to be as professonal as possible\n",
    "    * Do describe about the prediction the user asks about it.\n",
    "    * If the case is too severe.\n",
    "    * If somethings are unclear to you, ask the user to describe it and help the user describe it by guiding them.\n",
    "\n",
    "2. **Response Generation (as a valid JSON object with 'response', 'thinking', and 'reasoning'):**\n",
    "\n",
    "    * **If Input Contains Specific Symptoms:**\n",
    "        Output *only* the following valid JSON object:\n",
    "        ```json\n",
    "        {{\n",
    "          \"response\": \"...\",\n",
    "          \"thinking\": \"...\",\n",
    "          \"reasoning\": \"...\"\n",
    "        }}\n",
    "        ```\n",
    "        For the `\"response\"`:\n",
    "        a. Acknowledge the user's symptoms empathetically.\n",
    "        b. Identify the *single most plausible* condition suggested by the combination of symptoms and context. If multiple seem equally likely based on context, mention the top one or two possibilities briefly.\n",
    "        c. Ask 1-2 specific, relevant follow-up questions to gather more details.\n",
    "        For the `\"thinking\"`:\n",
    "        d. Briefly explain that you considered the symptoms alongside the provided Context Cases.\n",
    "        For the `\"reasoning\"`:\n",
    "        e. Explain your reasoning concisely, referencing how the user's symptoms relate to the context if there's a clear link.\n",
    "\n",
    "    * **If Input is Vague/General (e.g., \"I feel unwell\"):**\n",
    "        Output *only* the following valid JSON object:\n",
    "        ```json\n",
    "        {{\n",
    "          \"response\": \"...\",\n",
    "          \"thinking\": \"...\",\n",
    "          \"reasoning\": \"...\"\n",
    "        }}\n",
    "        ```\n",
    "        For the `\"response\"`:\n",
    "        a. Respond empathetically, acknowledging they don't feel well.\n",
    "        b. Gently guide them to provide more specific information by asking open-ended questions. For example: \"I understand you're feeling unwell. Could you tell me more about what specific symptoms you are experiencing? For instance, are you feeling any pain, fatigue, fever, or anything else in particular?\"\n",
    "        For the `\"thinking\"`:\n",
    "        Indicate that the input was vague or general and more specific information is needed to provide relevant context.\n",
    "        For the `\"reasoning\"`:\n",
    "        State that a potential condition cannot be identified based on the vague input alone.\n",
    "\n",
    "    * **If Input is Conversational/Unrelated (e.g., \"hello\"):**\n",
    "        Output *only* the following valid JSON object:\n",
    "        ```json\n",
    "        {{\n",
    "          \"response\": \"...\",\n",
    "          \"thinking\": \"...\",\n",
    "          \"reasoning\": \"...\"\n",
    "        }}\n",
    "        ```\n",
    "        For the `\"response\"`:\n",
    "        a. Respond in a friendly and human-like manner. For example: \"Hello! I hope you're doing well. If you have any health concerns or are experiencing any symptoms you'd like to discuss, please feel free to share them with me. I'm here to help provide some information based on the data I have.\"\n",
    "        b. Briefly explain your purpose and encourage them to share health-related information.\n",
    "        For the `\"thinking\"`:\n",
    "        Indicate that the input was conversational or unrelated to health and a health-related query is expected.\n",
    "        For the `\"reasoning\"`:\n",
    "        State that the input was not a description of health symptoms.\n",
    "\n",
    "**Output Style:** Ensure the output is a *valid JSON object* with the specified fields ('response', 'thinking', 'reasoning') and without any additional text or markdown.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        response_text = gemini_model.generate_content(prompt).text\n",
    "\n",
    "        if response_text.startswith(\"```json\") and response_text.endswith(\"```\"):\n",
    "            response_text = response_text[7:-3].strip()  \n",
    "\n",
    "        try:\n",
    "            response_json = json.loads(response_text)\n",
    "            return response_json\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON Decode Error: {e}\")\n",
    "            print(f\"Raw Response: {response_text}\")\n",
    "            return {\"response_type\": \"error\", \"message\": \"Error decoding JSON response from the model.\", \"raw_response\": response_text}\n",
    "    except Exception as e:\n",
    "        return {\"response_type\": \"error\", \"message\": f\"An error occurred while calling the Gemini API: {e}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaae4f4-270a-423b-828d-25de51a33376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"ola amigo\"\n",
    "prediction = predict_disease(user_input)\n",
    "print(json.dumps(prediction, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
